1 - I believe that the better solution for reusing the data generated in the first Jupyter file, in the case of a .CSV, can be to save it in an RDBMS table, whether in an on-premises environment or RDS cloud, thus facilitating the use of the result of the solution by any other contributor or application;

1.1 - Suggested pipeline can be for e.g: generate the result of the Jupyter 1 script in a table of an RDBMS, MySQL in my example, however, it could be Oracle, SQL Server, Postgres, on-premises or in a cloud environment;

# This bellow is the better in my opinion (but I've not environment to create)
1.2 - Another option that I developed in our example cloud, can store the .CSV result of Jupyter 1 in HDFS of Hadoop, in a Cassandra Keyspace, Mongodb document or in an AWS S3 Bucket that was my choice, where the API would be made via AWS CLI, importing credentials from the Python script for any need to use the solution;

Scenario 1.2 (AWS S3 example for Redshift) : - Credentials.py and dataingestion.py file --> "respecting data security, we can create the scripts in separate directories and only the System user has read credentials in that directory of the credentials"

--> At the end of the file generated by Jupyter 1, we could add:

! aws s3 mb s3://bucket_analytics/csv_files/

! aws s3 cp train_model.csv s3://bucket_analytics/csv_files/

## In the end what we will have would be:

1- An S3 bucket on AWS with the train_model.csv file
2- A fully parameterized Python script with credential security, which accesses this bucket and reads our generated .CSV file
3- This same Python script creates a table inside Redshift on AWS and inserts the data from the file
4- Through this table, other users of the company or applications and visualization tools with Quicksight, Tableau, PowerBI, Python, SQL can work with the generated data
5- The pipeline can be Schedulated with Jenkins (CI/CD)
6- Example Jenkins configuration:
6.1 - Step by step
- Access Jenkins
- Create pipeline: creditrisk
- Configure with Git under "Source code management"
- Schedular with "Build Trigger" (every 5 minutes, for example, or every 15min, 30min, 1 hour, etc)
- Build --> Run Shell (Point to Python.bat path)

## I've other sugestions such as:
1- Use Terraform to create a buckets;
2- Maybe we can use Airflow against the Jenkins to Schedule and maintenance pipeline jobs/projects;
3- We can use any NoSQL database for more performatic process when we talking about Systems consuming data and the problems for "use" the data sets can be solved using e.g a Cassandradb, because we can create the process such as a SQL databases and we can make SQL selects in the Cassandra too;
